{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnRrow4E6oNeg/a/i8TNzu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhinavMekala/ML_F_PES2UG23CS338_MB-ABHINAV/blob/main/EC_F_PES2UG23CS338_Lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSFDjPjoblxC",
        "outputId": "398ff008-cf6b-4f17-9cb4-68fb10d79569"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing EC_F_PES2UG23CS338_Lab3.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile EC_F_PES2UG23CS338_Lab3.py\n",
        "\n",
        "import torch\n",
        "def get_entropy_of_dataset(tensor: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Calculate the entropy of the entire dataset.\n",
        "    Formula: Entropy = -Σ(p_i * log2(p_i)) where p_i is the probability of class i\n",
        "\n",
        "    Args:\n",
        "        tensor (torch.Tensor): Input dataset as a tensor, where the last column is the target.\n",
        "\n",
        "    Returns:\n",
        "        float: Entropy of the dataset.\n",
        "    \"\"\"\n",
        "    # TODO: Implement this function\n",
        "    target_col = tensor[:, -1]\n",
        "    unique_classes, counts = torch.unique(target_col, return_counts=True)\n",
        "    probabilities = counts.float() / target_col.size(0)\n",
        "    probabilities = probabilities[probabilities > 0]\n",
        "    entropy = -torch.sum(probabilities * torch.log2(probabilities))\n",
        "    return entropy.item()\n",
        "\n",
        "def get_avg_info_of_attribute(tensor: torch.Tensor, attribute: int):\n",
        "    \"\"\"\n",
        "    Calculate the average information (weighted entropy) of an attribute.\n",
        "    Formula: Avg_Info = Σ((|S_v|/|S|) * Entropy(S_v)) where S_v is subset with attribute value v.\n",
        "\n",
        "    Args:\n",
        "        tensor (torch.Tensor): Input dataset as a tensor.\n",
        "        attribute (int): Index of the attribute column.\n",
        "\n",
        "    Returns:\n",
        "        float: Average information of the attribute.\n",
        "    \"\"\"\n",
        "    # TODO: Implement this function\n",
        "    target_col = tensor[:, -1]\n",
        "    unique_values, counts = torch.unique(tensor[:, attribute], return_counts=True)\n",
        "    total_count = tensor.size(0)\n",
        "    avg_info = 0.0\n",
        "    for value, count in zip(unique_values, counts):\n",
        "        subset = tensor[tensor[:, attribute] == value]\n",
        "        subset_entropy = get_entropy_of_dataset(subset)\n",
        "        avg_info += (count.float() / total_count) * subset_entropy\n",
        "    return avg_info.item()\n",
        "\n",
        "def get_information_gain(tensor: torch.Tensor, attribute: int):\n",
        "    \"\"\"\n",
        "    Calculate Information Gain for an attribute.\n",
        "    Formula: Information_Gain = Entropy(S) - Avg_Info(attribute)\n",
        "\n",
        "    Args:\n",
        "        tensor (torch.Tensor): Input dataset as a tensor.\n",
        "        attribute (int): Index of the attribute column.\n",
        "\n",
        "    Returns:\n",
        "        float: Information gain for the attribute (rounded to 4 decimals).\n",
        "    \"\"\"\n",
        "    # TODO: Implement this function\n",
        "    entropy = get_entropy_of_dataset(tensor)\n",
        "    avg_info = get_avg_info_of_attribute(tensor, attribute)\n",
        "    information_gain = entropy - avg_info\n",
        "    return round(information_gain, 4)\n",
        "\n",
        "def get_selected_attribute(tensor: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Select the best attribute based on highest information gain.\n",
        "\n",
        "    Returns a tuple with:\n",
        "    1. Dictionary mapping attribute indices to their information gains\n",
        "    2. Index of the attribute with highest information gain\n",
        "\n",
        "    Example: ({0: 0.123, 1: 0.768, 2: 1.23}, 2)\n",
        "\n",
        "    Args:\n",
        "        tensor (torch.Tensor): Input dataset as a tensor.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (dict of attribute:index -> information gain, index of best attribute)\n",
        "    \"\"\"\n",
        "    # TODO: Implement this function\n",
        "    information_gains = {}\n",
        "    for attribute in range(tensor.size(1) - 1):  # Exclude target column\n",
        "        information_gains[attribute] = get_information_gain(tensor, attribute)\n",
        "    best_attribute = max(information_gains, key=information_gains.get)\n",
        "    return information_gains, best_attribute"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 test.py --ID EC_F_PES2UG23CS338_Lab3 --data mushrooms.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDmXnGwAc_xE",
        "outputId": "a6b5e81e-3b1f-4007-8e7c-4d81629e4ff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running tests with PYTORCH framework\n",
            "============================================================\n",
            " target column: 'class' (last column)\n",
            "Original dataset info:\n",
            "Shape: (8124, 23)\n",
            "Columns: ['cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring', 'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number', 'ring-type', 'spore-print-color', 'population', 'habitat', 'class']\n",
            "\n",
            "First few rows:\n",
            "\n",
            "cap-shape: ['x' 'b' 's' 'f' 'k'] -> [5 0 4 2 3]\n",
            "\n",
            "cap-surface: ['s' 'y' 'f' 'g'] -> [2 3 0 1]\n",
            "\n",
            "cap-color: ['n' 'y' 'w' 'g' 'e'] -> [4 9 8 3 2]\n",
            "\n",
            "class: ['p' 'e'] -> [1 0]\n",
            "\n",
            "Processed dataset shape: torch.Size([8124, 23])\n",
            "Number of features: 22\n",
            "Features: ['cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring', 'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number', 'ring-type', 'spore-print-color', 'population', 'habitat']\n",
            "Target: class\n",
            "Framework: PYTORCH\n",
            "Data type: <class 'torch.Tensor'>\n",
            "\n",
            "============================================================\n",
            "DECISION TREE CONSTRUCTION DEMO\n",
            "============================================================\n",
            "Total samples: 8124\n",
            "Training samples: 6499\n",
            "Testing samples: 1625\n",
            "\n",
            "Constructing decision tree using training data...\n",
            "\n",
            "🌳 Decision tree construction completed using PYTORCH!\n",
            "\n",
            "📊 OVERALL PERFORMANCE METRICS\n",
            "========================================\n",
            "Accuracy:             1.0000 (100.00%)\n",
            "Precision (weighted): 1.0000\n",
            "Recall (weighted):    1.0000\n",
            "F1-Score (weighted):  1.0000\n",
            "Precision (macro):    1.0000\n",
            "Recall (macro):       1.0000\n",
            "F1-Score (macro):     1.0000\n",
            "\n",
            "🌳 TREE COMPLEXITY METRICS\n",
            "========================================\n",
            "Maximum Depth:        4\n",
            "Total Nodes:          29\n",
            "Leaf Nodes:           24\n",
            "Internal Nodes:       5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 test.py --ID EC_F_PES2UG23CS338_Lab3 --data Nursery.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QC6QJajAdbT6",
        "outputId": "5b644ef6-54ad-4997-d94c-40a95d1f1724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running tests with PYTORCH framework\n",
            "============================================================\n",
            " target column: 'class' (last column)\n",
            "Original dataset info:\n",
            "Shape: (12960, 9)\n",
            "Columns: ['parents', 'has_nurs', 'form', 'children', 'housing', 'finance', 'social', 'health', 'class']\n",
            "\n",
            "First few rows:\n",
            "\n",
            "parents: ['usual' 'pretentious' 'great_pret'] -> [2 1 0]\n",
            "\n",
            "has_nurs: ['proper' 'less_proper' 'improper' 'critical' 'very_crit'] -> [3 2 1 0 4]\n",
            "\n",
            "form: ['complete' 'completed' 'incomplete' 'foster'] -> [0 1 3 2]\n",
            "\n",
            "class: ['recommend' 'priority' 'not_recom' 'very_recom' 'spec_prior'] -> [2 1 0 4 3]\n",
            "\n",
            "Processed dataset shape: torch.Size([12960, 9])\n",
            "Number of features: 8\n",
            "Features: ['parents', 'has_nurs', 'form', 'children', 'housing', 'finance', 'social', 'health']\n",
            "Target: class\n",
            "Framework: PYTORCH\n",
            "Data type: <class 'torch.Tensor'>\n",
            "\n",
            "============================================================\n",
            "DECISION TREE CONSTRUCTION DEMO\n",
            "============================================================\n",
            "Total samples: 12960\n",
            "Training samples: 10368\n",
            "Testing samples: 2592\n",
            "\n",
            "Constructing decision tree using training data...\n",
            "\n",
            "🌳 Decision tree construction completed using PYTORCH!\n",
            "\n",
            "📊 OVERALL PERFORMANCE METRICS\n",
            "========================================\n",
            "Accuracy:             0.9867 (98.67%)\n",
            "Precision (weighted): 0.9876\n",
            "Recall (weighted):    0.9867\n",
            "F1-Score (weighted):  0.9872\n",
            "Precision (macro):    0.7604\n",
            "Recall (macro):       0.7654\n",
            "F1-Score (macro):     0.7628\n",
            "\n",
            "🌳 TREE COMPLEXITY METRICS\n",
            "========================================\n",
            "Maximum Depth:        7\n",
            "Total Nodes:          952\n",
            "Leaf Nodes:           680\n",
            "Internal Nodes:       272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 test.py --ID EC_F_PES2UG23CS338_Lab3 --data tictactoe.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeGMlLDwd8Or",
        "outputId": "b8937594-7c5c-4af4-d41c-3781e1f53470"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running tests with PYTORCH framework\n",
            "============================================================\n",
            " target column: 'Class' (last column)\n",
            "Original dataset info:\n",
            "Shape: (958, 10)\n",
            "Columns: ['top-left-square', 'top-middle-square', 'top-right-square', 'middle-left-square', 'middle-middle-square', 'middle-right-square', 'bottom-left-square', 'bottom-middle-square', 'bottom-right-square', 'Class']\n",
            "\n",
            "First few rows:\n",
            "\n",
            "top-left-square: ['x' 'o' 'b'] -> [2 1 0]\n",
            "\n",
            "top-middle-square: ['x' 'o' 'b'] -> [2 1 0]\n",
            "\n",
            "top-right-square: ['x' 'o' 'b'] -> [2 1 0]\n",
            "\n",
            "Class: ['positive' 'negative'] -> [1 0]\n",
            "\n",
            "Processed dataset shape: torch.Size([958, 10])\n",
            "Number of features: 9\n",
            "Features: ['top-left-square', 'top-middle-square', 'top-right-square', 'middle-left-square', 'middle-middle-square', 'middle-right-square', 'bottom-left-square', 'bottom-middle-square', 'bottom-right-square']\n",
            "Target: Class\n",
            "Framework: PYTORCH\n",
            "Data type: <class 'torch.Tensor'>\n",
            "\n",
            "============================================================\n",
            "DECISION TREE CONSTRUCTION DEMO\n",
            "============================================================\n",
            "Total samples: 958\n",
            "Training samples: 766\n",
            "Testing samples: 192\n",
            "\n",
            "Constructing decision tree using training data...\n",
            "\n",
            "🌳 Decision tree construction completed using PYTORCH!\n",
            "\n",
            "📊 OVERALL PERFORMANCE METRICS\n",
            "========================================\n",
            "Accuracy:             0.8730 (87.30%)\n",
            "Precision (weighted): 0.8741\n",
            "Recall (weighted):    0.8730\n",
            "F1-Score (weighted):  0.8734\n",
            "Precision (macro):    0.8590\n",
            "Recall (macro):       0.8638\n",
            "F1-Score (macro):     0.8613\n",
            "\n",
            "🌳 TREE COMPLEXITY METRICS\n",
            "========================================\n",
            "Maximum Depth:        7\n",
            "Total Nodes:          281\n",
            "Leaf Nodes:           180\n",
            "Internal Nodes:       101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sPV9eXOxeDPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Performance Comparison\n",
        "Nursery Dataset:\n",
        "o Large dataset, highly multi-class, with strong relationships between\n",
        "family/finance attributes and target.\n",
        "o Accuracy: Approx 90% achievable with trees, but may drop slightly due to\n",
        "many class labels.\n",
        "o Precision/Recall/F1: Lower than mushrooms because of imbalanced class\n",
        "distribution.\n",
        "\n",
        "Mushrooms Dataset:\n",
        "o Clean, binary classification (edible vs poisonous).\n",
        "o Certain features like odor split the dataset almost perfectly.\n",
        "o Accuracy: Close to 100%.\n",
        "o Precision/Recall/F1: Near-perfect, since splits strongly correlate with the\n",
        "class.\n",
        "\n",
        "Tic-Tac-Toe Dataset:\n",
        "o Medium-sized, binary target.\n",
        "o Accuracy: Around 85–90%, since patterns are deterministic but\n",
        "representation is discrete.\n",
        "o Precision/Recall/F1: Balanced, but sometimes misclassifies unusual board\n",
        "states.\n",
        "Ranking by performance, Mushrooms greater than Nursery. Nursery and Tic Tac Toe\n",
        "are almost the same.\n",
        "\n",
        "\n",
        "2. Tree Characteristics Analysis\n",
        "Tree Depth:\n",
        "o Nursery: Deep. Depth around 10+.\n",
        "o Mushrooms: Shallow, since features like odor separate almost immediately.\n",
        "Depth approximately 4–6.\n",
        "o Tic-Tac-Toe: Medium depth, depending on win conditions.\n",
        "\n",
        "Number of Nodes:\n",
        "o Nursery: High, due to many attributes and values.\n",
        "o Mushrooms: Low, since 1–2 key features decide most splits.\n",
        "o Tic-Tac-Toe: Medium, correlating with possible winning states.\n",
        "\n",
        "Most Important Features:\n",
        "o Nursery: finance, family, and social factors are usually top splits.\n",
        "o Mushrooms: odor, gill size, spore print color.\n",
        "o Tic-Tac-Toe: Central square, followed by corners.\n",
        "\n",
        "Tree Complexity:\n",
        "o Nursery: High complexity.\n",
        "o Mushrooms: Low complexity.\n",
        "o Tic-Tac-Toe: Medium complexity.\n",
        "\n",
        "\n",
        "3. Dataset-Specific Insights\n",
        "Nursery Dataset:\n",
        "• Feature Importance: Financial stability and parental preference dominate\n",
        "splits.\n",
        "• Class Distribution: Imbalanced (some decisions like “recommend” are rare).\n",
        "• Decision Patterns: “Good financial + supportive family → priority admission.”\n",
        "• Overfitting: Risk is higher due to many features/values; pruning is necessary.\n",
        "Mushrooms Dataset:\n",
        "• Feature Importance: Odor is the single strongest indicator (almost perfect\n",
        "split).\n",
        "• Class Distribution: Balanced (edible vs poisonous).\n",
        "• Decision Patterns: “Foul odor means poisonous” emerges early.\n",
        "• Overfitting: Minimal, since dataset is clean and separable.\n",
        "Tic-Tac-Toe Dataset:\n",
        "• Feature Importance: Center cell is most predictive.\n",
        "• Class Distribution: Slight imbalance depending on X/O placements.\n",
        "• Decision Patterns: “X in center and X in corner implies positive outcome.”\n",
        "• Overfitting: Possible if tree memorizes board positions rather than general\n",
        "patterns.\n",
        "\n",
        "\n",
        "4. Comparative Analysis Report\n",
        "a) Algorithm Performance:\n",
        "• Highest Accuracy: Mushrooms dataset, because of strong attribute-class\n",
        "correlation.\n",
        "• Dataset Size Effect: Nursery implies longer training time, deeper trees.\n",
        "Mushrooms imply efficient, clean splits. Tic-Tac-Toe implies manageable.\n",
        "• Role of Features: More features (nursery) increase tree depth and complexity,\n",
        "while fewer decisive features (mushrooms) yield simpler, more accurate trees.\n",
        "\n",
        "b) Data Characteristics Impact:\n",
        "• Class Imbalance: Nursery suffers from imbalance, some minority classes\n",
        "harder to predict. Mushrooms balanced, strong results. Tic-Tac-Toe\n",
        "moderately imbalanced.\n",
        "• Feature Types: Binary features (mushrooms, tic-tac-toe) lead to cleaner splits.\n",
        "Multi-valued categorical features (nursery), more complex splits, higher\n",
        "overfitting risk.\n",
        "\n",
        "c) Practical Applications:\n",
        "• Nursery: Admission decision support systems. Interpretable, but may need\n",
        "pruning for real-world use.\n",
        "• Mushrooms: Food safety classification. High accuracy, easily interpretable rules.\n",
        "• Tic-Tac-Toe: Game AI explanation. Good interpretability for explaining strategies.\n",
        "\n",
        "\n",
        "Interpretability Advantages:\n",
        "• Nursery: Explains how financial/social factors affect admission.\n",
        "• Mushrooms: Clear, human-readable rules.\n",
        "• Tic-Tac-Toe: Explains why certain moves are critical.\n",
        "\n",
        "\n",
        "Improvements:\n",
        "• Nursery: Use tree pruning and possibly convert to Random Forest to reduce\n",
        "overfitting.\n",
        "• Mushrooms: Already near-perfect, little improvement needed. Could compress\n",
        "rules.\n",
        "• Tic-Tac-Toe: Use feature engineering to reduce depth.\n",
        "\n",
        "\n",
        "Observation:\n",
        "• Mushrooms dataset has the highest accuracy, simplest tree.\n",
        "• Nursery dataset has the largest, complex tree, risk of overfitting.\n",
        "• Tic-Tac-Toe dataset has medium complexity, interpretable decision paths."
      ],
      "metadata": {
        "id": "X3eMUy5xvjIu"
      }
    }
  ]
}
